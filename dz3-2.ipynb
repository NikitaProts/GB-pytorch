{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e526eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "california_housing = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf27d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import Tensor\n",
    "from sklearn.preprocessing import MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605b9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e7b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = california_housing.data\n",
    "target = california_housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f53247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fc3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159f583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    \n",
    "train_dataset = RegressionDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "test_dataset = RegressionDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc855c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c16f9c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653c1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99760ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleRegression(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(MultipleRegression, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 16)\n",
    "        self.bn = nn.BatchNorm1d(16)\n",
    "        self.dp = nn.Dropout(0.25)\n",
    "        self.layer_out = nn.Linear(16, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)\n",
    "    \n",
    "    def predict(self, test_inputs):\n",
    "        x = self.relu(self.layer_1(test_inputs))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd33b72",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de547ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleRegression(\n",
      "  (layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp): Dropout(p=0.25, inplace=False)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultipleRegression(NUM_FEATURES)\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36828087",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6866cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0406e484602d4340a5e371cb9e5b5bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 2.69493\n",
      "Epoch 002: | Train Loss: 1.17434\n",
      "Epoch 003: | Train Loss: 1.00337\n",
      "Epoch 004: | Train Loss: 0.79697\n",
      "Epoch 005: | Train Loss: 0.64698\n",
      "Epoch 006: | Train Loss: 0.58835\n",
      "Epoch 007: | Train Loss: 0.57130\n",
      "Epoch 008: | Train Loss: 0.56377\n",
      "Epoch 009: | Train Loss: 0.55787\n",
      "Epoch 010: | Train Loss: 0.55236\n",
      "Epoch 011: | Train Loss: 0.54761\n",
      "Epoch 012: | Train Loss: 0.54276\n",
      "Epoch 013: | Train Loss: 0.53941\n",
      "Epoch 014: | Train Loss: 0.53491\n",
      "Epoch 015: | Train Loss: 0.53198\n",
      "Epoch 016: | Train Loss: 0.52752\n",
      "Epoch 017: | Train Loss: 0.52398\n",
      "Epoch 018: | Train Loss: 0.52027\n",
      "Epoch 019: | Train Loss: 0.51769\n",
      "Epoch 020: | Train Loss: 0.51455\n",
      "Epoch 021: | Train Loss: 0.51279\n",
      "Epoch 022: | Train Loss: 0.50951\n",
      "Epoch 023: | Train Loss: 0.50725\n",
      "Epoch 024: | Train Loss: 0.50499\n",
      "Epoch 025: | Train Loss: 0.50376\n",
      "Epoch 026: | Train Loss: 0.50170\n",
      "Epoch 027: | Train Loss: 0.49947\n",
      "Epoch 028: | Train Loss: 0.49864\n",
      "Epoch 029: | Train Loss: 0.49611\n",
      "Epoch 030: | Train Loss: 0.49500\n",
      "Epoch 031: | Train Loss: 0.49322\n",
      "Epoch 032: | Train Loss: 0.49162\n",
      "Epoch 033: | Train Loss: 0.49068\n",
      "Epoch 034: | Train Loss: 0.48847\n",
      "Epoch 035: | Train Loss: 0.48869\n",
      "Epoch 036: | Train Loss: 0.48551\n",
      "Epoch 037: | Train Loss: 0.48410\n",
      "Epoch 038: | Train Loss: 0.48274\n",
      "Epoch 039: | Train Loss: 0.48093\n",
      "Epoch 040: | Train Loss: 0.47990\n",
      "Epoch 041: | Train Loss: 0.47823\n",
      "Epoch 042: | Train Loss: 0.47750\n",
      "Epoch 043: | Train Loss: 0.47580\n",
      "Epoch 044: | Train Loss: 0.47499\n",
      "Epoch 045: | Train Loss: 0.47419\n",
      "Epoch 046: | Train Loss: 0.47199\n",
      "Epoch 047: | Train Loss: 0.47165\n",
      "Epoch 048: | Train Loss: 0.47059\n",
      "Epoch 049: | Train Loss: 0.47012\n",
      "Epoch 050: | Train Loss: 0.46786\n",
      "Epoch 051: | Train Loss: 0.46768\n",
      "Epoch 052: | Train Loss: 0.46640\n",
      "Epoch 053: | Train Loss: 0.46645\n",
      "Epoch 054: | Train Loss: 0.46525\n",
      "Epoch 055: | Train Loss: 0.46375\n",
      "Epoch 056: | Train Loss: 0.46258\n",
      "Epoch 057: | Train Loss: 0.46339\n",
      "Epoch 058: | Train Loss: 0.46142\n",
      "Epoch 059: | Train Loss: 0.46113\n",
      "Epoch 060: | Train Loss: 0.46039\n",
      "Epoch 061: | Train Loss: 0.45926\n",
      "Epoch 062: | Train Loss: 0.45882\n",
      "Epoch 063: | Train Loss: 0.45851\n",
      "Epoch 064: | Train Loss: 0.45716\n",
      "Epoch 065: | Train Loss: 0.45620\n",
      "Epoch 066: | Train Loss: 0.45632\n",
      "Epoch 067: | Train Loss: 0.45562\n",
      "Epoch 068: | Train Loss: 0.45402\n",
      "Epoch 069: | Train Loss: 0.45398\n",
      "Epoch 070: | Train Loss: 0.45409\n",
      "Epoch 071: | Train Loss: 0.45313\n",
      "Epoch 072: | Train Loss: 0.45233\n",
      "Epoch 073: | Train Loss: 0.45180\n",
      "Epoch 074: | Train Loss: 0.45254\n",
      "Epoch 075: | Train Loss: 0.45011\n",
      "Epoch 076: | Train Loss: 0.44899\n",
      "Epoch 077: | Train Loss: 0.44870\n",
      "Epoch 078: | Train Loss: 0.44834\n",
      "Epoch 079: | Train Loss: 0.44790\n",
      "Epoch 080: | Train Loss: 0.44754\n",
      "Epoch 081: | Train Loss: 0.44646\n",
      "Epoch 082: | Train Loss: 0.44578\n",
      "Epoch 083: | Train Loss: 0.44477\n",
      "Epoch 084: | Train Loss: 0.44574\n",
      "Epoch 085: | Train Loss: 0.44444\n",
      "Epoch 086: | Train Loss: 0.44434\n",
      "Epoch 087: | Train Loss: 0.44305\n",
      "Epoch 088: | Train Loss: 0.44348\n",
      "Epoch 089: | Train Loss: 0.44186\n",
      "Epoch 090: | Train Loss: 0.44117\n",
      "Epoch 091: | Train Loss: 0.44136\n",
      "Epoch 092: | Train Loss: 0.44091\n",
      "Epoch 093: | Train Loss: 0.43976\n",
      "Epoch 094: | Train Loss: 0.43973\n",
      "Epoch 095: | Train Loss: 0.43924\n",
      "Epoch 096: | Train Loss: 0.43907\n",
      "Epoch 097: | Train Loss: 0.43807\n",
      "Epoch 098: | Train Loss: 0.43712\n",
      "Epoch 099: | Train Loss: 0.43761\n",
      "Epoch 100: | Train Loss: 0.43686\n",
      "Epoch 101: | Train Loss: 0.43693\n",
      "Epoch 102: | Train Loss: 0.43562\n",
      "Epoch 103: | Train Loss: 0.43685\n",
      "Epoch 104: | Train Loss: 0.43473\n",
      "Epoch 105: | Train Loss: 0.43449\n",
      "Epoch 106: | Train Loss: 0.43444\n",
      "Epoch 107: | Train Loss: 0.43371\n",
      "Epoch 108: | Train Loss: 0.43314\n",
      "Epoch 109: | Train Loss: 0.43385\n",
      "Epoch 110: | Train Loss: 0.43356\n",
      "Epoch 111: | Train Loss: 0.43229\n",
      "Epoch 112: | Train Loss: 0.43181\n",
      "Epoch 113: | Train Loss: 0.43112\n",
      "Epoch 114: | Train Loss: 0.43091\n",
      "Epoch 115: | Train Loss: 0.43064\n",
      "Epoch 116: | Train Loss: 0.42979\n",
      "Epoch 117: | Train Loss: 0.42977\n",
      "Epoch 118: | Train Loss: 0.42872\n",
      "Epoch 119: | Train Loss: 0.43054\n",
      "Epoch 120: | Train Loss: 0.42856\n",
      "Epoch 121: | Train Loss: 0.42798\n",
      "Epoch 122: | Train Loss: 0.42774\n",
      "Epoch 123: | Train Loss: 0.42688\n",
      "Epoch 124: | Train Loss: 0.42730\n",
      "Epoch 125: | Train Loss: 0.42530\n",
      "Epoch 126: | Train Loss: 0.42500\n",
      "Epoch 127: | Train Loss: 0.42570\n",
      "Epoch 128: | Train Loss: 0.42459\n",
      "Epoch 129: | Train Loss: 0.42426\n",
      "Epoch 130: | Train Loss: 0.42475\n",
      "Epoch 131: | Train Loss: 0.42390\n",
      "Epoch 132: | Train Loss: 0.42461\n",
      "Epoch 133: | Train Loss: 0.42217\n",
      "Epoch 134: | Train Loss: 0.42295\n",
      "Epoch 135: | Train Loss: 0.42187\n",
      "Epoch 136: | Train Loss: 0.42154\n",
      "Epoch 137: | Train Loss: 0.42158\n",
      "Epoch 138: | Train Loss: 0.42117\n",
      "Epoch 139: | Train Loss: 0.42111\n",
      "Epoch 140: | Train Loss: 0.41973\n",
      "Epoch 141: | Train Loss: 0.42042\n",
      "Epoch 142: | Train Loss: 0.41966\n",
      "Epoch 143: | Train Loss: 0.41912\n",
      "Epoch 144: | Train Loss: 0.42013\n",
      "Epoch 145: | Train Loss: 0.41893\n",
      "Epoch 146: | Train Loss: 0.41805\n",
      "Epoch 147: | Train Loss: 0.41877\n",
      "Epoch 148: | Train Loss: 0.41707\n",
      "Epoch 149: | Train Loss: 0.41652\n",
      "Epoch 150: | Train Loss: 0.41692\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "                           \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a58573b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in test_loader:\n",
    "       # X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_list.append(y_test_pred.numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f78b24ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 0.4403075666576879\n",
      "R^2 : 0.6741630321860955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred_list)\n",
    "r_square = r2_score(y_test, y_pred_list)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb02fbd8",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c19eded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleRegression(\n",
      "  (layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp): Dropout(p=0.25, inplace=False)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultipleRegression(NUM_FEATURES)\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15968132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482e8af073fa4487bf292af5dd5f07a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 1.63153\n",
      "Epoch 002: | Train Loss: 1.09556\n",
      "Epoch 003: | Train Loss: 0.92228\n",
      "Epoch 004: | Train Loss: 0.72390\n",
      "Epoch 005: | Train Loss: 0.61845\n",
      "Epoch 006: | Train Loss: 0.58950\n",
      "Epoch 007: | Train Loss: 0.57814\n",
      "Epoch 008: | Train Loss: 0.56904\n",
      "Epoch 009: | Train Loss: 0.56127\n",
      "Epoch 010: | Train Loss: 0.55377\n",
      "Epoch 011: | Train Loss: 0.54717\n",
      "Epoch 012: | Train Loss: 0.53945\n",
      "Epoch 013: | Train Loss: 0.53274\n",
      "Epoch 014: | Train Loss: 0.52735\n",
      "Epoch 015: | Train Loss: 0.52265\n",
      "Epoch 016: | Train Loss: 0.51765\n",
      "Epoch 017: | Train Loss: 0.51339\n",
      "Epoch 018: | Train Loss: 0.50911\n",
      "Epoch 019: | Train Loss: 0.50487\n",
      "Epoch 020: | Train Loss: 0.50113\n",
      "Epoch 021: | Train Loss: 0.49734\n",
      "Epoch 022: | Train Loss: 0.49454\n",
      "Epoch 023: | Train Loss: 0.49184\n",
      "Epoch 024: | Train Loss: 0.48830\n",
      "Epoch 025: | Train Loss: 0.48544\n",
      "Epoch 026: | Train Loss: 0.48295\n",
      "Epoch 027: | Train Loss: 0.48057\n",
      "Epoch 028: | Train Loss: 0.47833\n",
      "Epoch 029: | Train Loss: 0.47712\n",
      "Epoch 030: | Train Loss: 0.47486\n",
      "Epoch 031: | Train Loss: 0.47265\n",
      "Epoch 032: | Train Loss: 0.47122\n",
      "Epoch 033: | Train Loss: 0.46940\n",
      "Epoch 034: | Train Loss: 0.46805\n",
      "Epoch 035: | Train Loss: 0.46632\n",
      "Epoch 036: | Train Loss: 0.46490\n",
      "Epoch 037: | Train Loss: 0.46361\n",
      "Epoch 038: | Train Loss: 0.46174\n",
      "Epoch 039: | Train Loss: 0.46117\n",
      "Epoch 040: | Train Loss: 0.45978\n",
      "Epoch 041: | Train Loss: 0.45865\n",
      "Epoch 042: | Train Loss: 0.45791\n",
      "Epoch 043: | Train Loss: 0.45583\n",
      "Epoch 044: | Train Loss: 0.45476\n",
      "Epoch 045: | Train Loss: 0.45368\n",
      "Epoch 046: | Train Loss: 0.45271\n",
      "Epoch 047: | Train Loss: 0.45197\n",
      "Epoch 048: | Train Loss: 0.45070\n",
      "Epoch 049: | Train Loss: 0.44966\n",
      "Epoch 050: | Train Loss: 0.44869\n",
      "Epoch 051: | Train Loss: 0.44758\n",
      "Epoch 052: | Train Loss: 0.44748\n",
      "Epoch 053: | Train Loss: 0.44622\n",
      "Epoch 054: | Train Loss: 0.44513\n",
      "Epoch 055: | Train Loss: 0.44432\n",
      "Epoch 056: | Train Loss: 0.44384\n",
      "Epoch 057: | Train Loss: 0.44270\n",
      "Epoch 058: | Train Loss: 0.44161\n",
      "Epoch 059: | Train Loss: 0.44092\n",
      "Epoch 060: | Train Loss: 0.44069\n",
      "Epoch 061: | Train Loss: 0.43963\n",
      "Epoch 062: | Train Loss: 0.43875\n",
      "Epoch 063: | Train Loss: 0.43868\n",
      "Epoch 064: | Train Loss: 0.43778\n",
      "Epoch 065: | Train Loss: 0.43710\n",
      "Epoch 066: | Train Loss: 0.43702\n",
      "Epoch 067: | Train Loss: 0.43588\n",
      "Epoch 068: | Train Loss: 0.43531\n",
      "Epoch 069: | Train Loss: 0.43403\n",
      "Epoch 070: | Train Loss: 0.43423\n",
      "Epoch 071: | Train Loss: 0.43406\n",
      "Epoch 072: | Train Loss: 0.43278\n",
      "Epoch 073: | Train Loss: 0.43231\n",
      "Epoch 074: | Train Loss: 0.43187\n",
      "Epoch 075: | Train Loss: 0.43173\n",
      "Epoch 076: | Train Loss: 0.43103\n",
      "Epoch 077: | Train Loss: 0.43026\n",
      "Epoch 078: | Train Loss: 0.42997\n",
      "Epoch 079: | Train Loss: 0.42879\n",
      "Epoch 080: | Train Loss: 0.42974\n",
      "Epoch 081: | Train Loss: 0.42846\n",
      "Epoch 082: | Train Loss: 0.42869\n",
      "Epoch 083: | Train Loss: 0.42761\n",
      "Epoch 084: | Train Loss: 0.42761\n",
      "Epoch 085: | Train Loss: 0.42655\n",
      "Epoch 086: | Train Loss: 0.42597\n",
      "Epoch 087: | Train Loss: 0.42604\n",
      "Epoch 088: | Train Loss: 0.42559\n",
      "Epoch 089: | Train Loss: 0.42507\n",
      "Epoch 090: | Train Loss: 0.42502\n",
      "Epoch 091: | Train Loss: 0.42419\n",
      "Epoch 092: | Train Loss: 0.42430\n",
      "Epoch 093: | Train Loss: 0.42289\n",
      "Epoch 094: | Train Loss: 0.42288\n",
      "Epoch 095: | Train Loss: 0.42306\n",
      "Epoch 096: | Train Loss: 0.42262\n",
      "Epoch 097: | Train Loss: 0.42211\n",
      "Epoch 098: | Train Loss: 0.42190\n",
      "Epoch 099: | Train Loss: 0.42120\n",
      "Epoch 100: | Train Loss: 0.42087\n",
      "Epoch 101: | Train Loss: 0.42085\n",
      "Epoch 102: | Train Loss: 0.42033\n",
      "Epoch 103: | Train Loss: 0.41988\n",
      "Epoch 104: | Train Loss: 0.41976\n",
      "Epoch 105: | Train Loss: 0.41904\n",
      "Epoch 106: | Train Loss: 0.41856\n",
      "Epoch 107: | Train Loss: 0.41882\n",
      "Epoch 108: | Train Loss: 0.41867\n",
      "Epoch 109: | Train Loss: 0.41727\n",
      "Epoch 110: | Train Loss: 0.41736\n",
      "Epoch 111: | Train Loss: 0.41735\n",
      "Epoch 112: | Train Loss: 0.41739\n",
      "Epoch 113: | Train Loss: 0.41648\n",
      "Epoch 114: | Train Loss: 0.41566\n",
      "Epoch 115: | Train Loss: 0.41579\n",
      "Epoch 116: | Train Loss: 0.41545\n",
      "Epoch 117: | Train Loss: 0.41553\n",
      "Epoch 118: | Train Loss: 0.41528\n",
      "Epoch 119: | Train Loss: 0.41481\n",
      "Epoch 120: | Train Loss: 0.41482\n",
      "Epoch 121: | Train Loss: 0.41445\n",
      "Epoch 122: | Train Loss: 0.41346\n",
      "Epoch 123: | Train Loss: 0.41357\n",
      "Epoch 124: | Train Loss: 0.41329\n",
      "Epoch 125: | Train Loss: 0.41310\n",
      "Epoch 126: | Train Loss: 0.41308\n",
      "Epoch 127: | Train Loss: 0.41210\n",
      "Epoch 128: | Train Loss: 0.41140\n",
      "Epoch 129: | Train Loss: 0.41190\n",
      "Epoch 130: | Train Loss: 0.41112\n",
      "Epoch 131: | Train Loss: 0.41056\n",
      "Epoch 132: | Train Loss: 0.41029\n",
      "Epoch 133: | Train Loss: 0.40978\n",
      "Epoch 134: | Train Loss: 0.40990\n",
      "Epoch 135: | Train Loss: 0.40990\n",
      "Epoch 136: | Train Loss: 0.40971\n",
      "Epoch 137: | Train Loss: 0.40915\n",
      "Epoch 138: | Train Loss: 0.40926\n",
      "Epoch 139: | Train Loss: 0.40863\n",
      "Epoch 140: | Train Loss: 0.40898\n",
      "Epoch 141: | Train Loss: 0.40806\n",
      "Epoch 142: | Train Loss: 0.40795\n",
      "Epoch 143: | Train Loss: 0.40750\n",
      "Epoch 144: | Train Loss: 0.40774\n",
      "Epoch 145: | Train Loss: 0.40725\n",
      "Epoch 146: | Train Loss: 0.40686\n",
      "Epoch 147: | Train Loss: 0.40671\n",
      "Epoch 148: | Train Loss: 0.40639\n",
      "Epoch 149: | Train Loss: 0.40615\n",
      "Epoch 150: | Train Loss: 0.40649\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "                           \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d414f5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 0.4317965285878678\n",
      "R^2 : 0.6804613814483395\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in test_loader:\n",
    "       # X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_list.append(y_test_pred.numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_list)\n",
    "r_square = r2_score(y_test, y_pred_list)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896536e6",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aaebf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleRegression(\n",
      "  (layer_1): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dp): Dropout(p=0.25, inplace=False)\n",
      "  (layer_out): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultipleRegression(NUM_FEATURES)\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49a566e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c4e72f0c0b4ef8a60326df6c1d8e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 3.30525\n",
      "Epoch 002: | Train Loss: 1.46783\n",
      "Epoch 003: | Train Loss: 1.23946\n",
      "Epoch 004: | Train Loss: 1.21464\n",
      "Epoch 005: | Train Loss: 1.20050\n",
      "Epoch 006: | Train Loss: 1.18748\n",
      "Epoch 007: | Train Loss: 1.17406\n",
      "Epoch 008: | Train Loss: 1.16046\n",
      "Epoch 009: | Train Loss: 1.14619\n",
      "Epoch 010: | Train Loss: 1.13122\n",
      "Epoch 011: | Train Loss: 1.11596\n",
      "Epoch 012: | Train Loss: 1.09946\n",
      "Epoch 013: | Train Loss: 1.08250\n",
      "Epoch 014: | Train Loss: 1.06471\n",
      "Epoch 015: | Train Loss: 1.04634\n",
      "Epoch 016: | Train Loss: 1.02733\n",
      "Epoch 017: | Train Loss: 1.00764\n",
      "Epoch 018: | Train Loss: 0.98737\n",
      "Epoch 019: | Train Loss: 0.96666\n",
      "Epoch 020: | Train Loss: 0.94561\n",
      "Epoch 021: | Train Loss: 0.92440\n",
      "Epoch 022: | Train Loss: 0.90275\n",
      "Epoch 023: | Train Loss: 0.88149\n",
      "Epoch 024: | Train Loss: 0.86008\n",
      "Epoch 025: | Train Loss: 0.83911\n",
      "Epoch 026: | Train Loss: 0.81859\n",
      "Epoch 027: | Train Loss: 0.79853\n",
      "Epoch 028: | Train Loss: 0.77906\n",
      "Epoch 029: | Train Loss: 0.76042\n",
      "Epoch 030: | Train Loss: 0.74257\n",
      "Epoch 031: | Train Loss: 0.72569\n",
      "Epoch 032: | Train Loss: 0.70966\n",
      "Epoch 033: | Train Loss: 0.69487\n",
      "Epoch 034: | Train Loss: 0.68113\n",
      "Epoch 035: | Train Loss: 0.66851\n",
      "Epoch 036: | Train Loss: 0.65690\n",
      "Epoch 037: | Train Loss: 0.64634\n",
      "Epoch 038: | Train Loss: 0.63652\n",
      "Epoch 039: | Train Loss: 0.62817\n",
      "Epoch 040: | Train Loss: 0.62033\n",
      "Epoch 041: | Train Loss: 0.61342\n",
      "Epoch 042: | Train Loss: 0.60728\n",
      "Epoch 043: | Train Loss: 0.60184\n",
      "Epoch 044: | Train Loss: 0.59698\n",
      "Epoch 045: | Train Loss: 0.59276\n",
      "Epoch 046: | Train Loss: 0.58893\n",
      "Epoch 047: | Train Loss: 0.58566\n",
      "Epoch 048: | Train Loss: 0.58266\n",
      "Epoch 049: | Train Loss: 0.58012\n",
      "Epoch 050: | Train Loss: 0.57780\n",
      "Epoch 051: | Train Loss: 0.57576\n",
      "Epoch 052: | Train Loss: 0.57403\n",
      "Epoch 053: | Train Loss: 0.57237\n",
      "Epoch 054: | Train Loss: 0.57103\n",
      "Epoch 055: | Train Loss: 0.56936\n",
      "Epoch 056: | Train Loss: 0.56811\n",
      "Epoch 057: | Train Loss: 0.56694\n",
      "Epoch 058: | Train Loss: 0.56569\n",
      "Epoch 059: | Train Loss: 0.56487\n",
      "Epoch 060: | Train Loss: 0.56380\n",
      "Epoch 061: | Train Loss: 0.56292\n",
      "Epoch 062: | Train Loss: 0.56192\n",
      "Epoch 063: | Train Loss: 0.56096\n",
      "Epoch 064: | Train Loss: 0.56029\n",
      "Epoch 065: | Train Loss: 0.55960\n",
      "Epoch 066: | Train Loss: 0.55862\n",
      "Epoch 067: | Train Loss: 0.55782\n",
      "Epoch 068: | Train Loss: 0.55710\n",
      "Epoch 069: | Train Loss: 0.55632\n",
      "Epoch 070: | Train Loss: 0.55564\n",
      "Epoch 071: | Train Loss: 0.55506\n",
      "Epoch 072: | Train Loss: 0.55437\n",
      "Epoch 073: | Train Loss: 0.55380\n",
      "Epoch 074: | Train Loss: 0.55292\n",
      "Epoch 075: | Train Loss: 0.55248\n",
      "Epoch 076: | Train Loss: 0.55159\n",
      "Epoch 077: | Train Loss: 0.55117\n",
      "Epoch 078: | Train Loss: 0.55043\n",
      "Epoch 079: | Train Loss: 0.55005\n",
      "Epoch 080: | Train Loss: 0.54920\n",
      "Epoch 081: | Train Loss: 0.54877\n",
      "Epoch 082: | Train Loss: 0.54806\n",
      "Epoch 083: | Train Loss: 0.54738\n",
      "Epoch 084: | Train Loss: 0.54695\n",
      "Epoch 085: | Train Loss: 0.54621\n",
      "Epoch 086: | Train Loss: 0.54562\n",
      "Epoch 087: | Train Loss: 0.54508\n",
      "Epoch 088: | Train Loss: 0.54467\n",
      "Epoch 089: | Train Loss: 0.54385\n",
      "Epoch 090: | Train Loss: 0.54327\n",
      "Epoch 091: | Train Loss: 0.54269\n",
      "Epoch 092: | Train Loss: 0.54205\n",
      "Epoch 093: | Train Loss: 0.54160\n",
      "Epoch 094: | Train Loss: 0.54087\n",
      "Epoch 095: | Train Loss: 0.54051\n",
      "Epoch 096: | Train Loss: 0.54000\n",
      "Epoch 097: | Train Loss: 0.53953\n",
      "Epoch 098: | Train Loss: 0.53895\n",
      "Epoch 099: | Train Loss: 0.53853\n",
      "Epoch 100: | Train Loss: 0.53793\n",
      "Epoch 101: | Train Loss: 0.53744\n",
      "Epoch 102: | Train Loss: 0.53690\n",
      "Epoch 103: | Train Loss: 0.53660\n",
      "Epoch 104: | Train Loss: 0.53598\n",
      "Epoch 105: | Train Loss: 0.53551\n",
      "Epoch 106: | Train Loss: 0.53532\n",
      "Epoch 107: | Train Loss: 0.53446\n",
      "Epoch 108: | Train Loss: 0.53416\n",
      "Epoch 109: | Train Loss: 0.53357\n",
      "Epoch 110: | Train Loss: 0.53338\n",
      "Epoch 111: | Train Loss: 0.53330\n",
      "Epoch 112: | Train Loss: 0.53242\n",
      "Epoch 113: | Train Loss: 0.53209\n",
      "Epoch 114: | Train Loss: 0.53159\n",
      "Epoch 115: | Train Loss: 0.53126\n",
      "Epoch 116: | Train Loss: 0.53089\n",
      "Epoch 117: | Train Loss: 0.53049\n",
      "Epoch 118: | Train Loss: 0.53017\n",
      "Epoch 119: | Train Loss: 0.52966\n",
      "Epoch 120: | Train Loss: 0.52945\n",
      "Epoch 121: | Train Loss: 0.52891\n",
      "Epoch 122: | Train Loss: 0.52854\n",
      "Epoch 123: | Train Loss: 0.52825\n",
      "Epoch 124: | Train Loss: 0.52764\n",
      "Epoch 125: | Train Loss: 0.52751\n",
      "Epoch 126: | Train Loss: 0.52706\n",
      "Epoch 127: | Train Loss: 0.52668\n",
      "Epoch 128: | Train Loss: 0.52652\n",
      "Epoch 129: | Train Loss: 0.52602\n",
      "Epoch 130: | Train Loss: 0.52560\n",
      "Epoch 131: | Train Loss: 0.52526\n",
      "Epoch 132: | Train Loss: 0.52493\n",
      "Epoch 133: | Train Loss: 0.52469\n",
      "Epoch 134: | Train Loss: 0.52428\n",
      "Epoch 135: | Train Loss: 0.52396\n",
      "Epoch 136: | Train Loss: 0.52367\n",
      "Epoch 137: | Train Loss: 0.52338\n",
      "Epoch 138: | Train Loss: 0.52296\n",
      "Epoch 139: | Train Loss: 0.52278\n",
      "Epoch 140: | Train Loss: 0.52233\n",
      "Epoch 141: | Train Loss: 0.52205\n",
      "Epoch 142: | Train Loss: 0.52164\n",
      "Epoch 143: | Train Loss: 0.52142\n",
      "Epoch 144: | Train Loss: 0.52104\n",
      "Epoch 145: | Train Loss: 0.52080\n",
      "Epoch 146: | Train Loss: 0.52057\n",
      "Epoch 147: | Train Loss: 0.52026\n",
      "Epoch 148: | Train Loss: 0.51998\n",
      "Epoch 149: | Train Loss: 0.51955\n",
      "Epoch 150: | Train Loss: 0.51936\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "                           \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ff732fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 0.532564653763472\n",
      "R^2 : 0.6058908247605476\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in test_loader:\n",
    "       # X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_list.append(y_test_pred.numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_list)\n",
    "r_square = r2_score(y_test, y_pred_list)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d490c3c",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "В данных наблюдениях лучше всех показал себя RMSprop с его r2= 0.68...  <br>\n",
    "Второе место Adam r2 = 0.67 <br>\n",
    "Третье SGD r2 = 0.60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a0b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acc4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
